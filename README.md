# U-pre
Using U-net1D to achieve significant results in time series prediction

This project, named U-pre, is led by Selen at Xidian and focuses on time series prediction. After investigating time series forecasting tasks, Selen identified that the input and output of time series prediction exhibit the same distribution, high correlation, and identical dimensionality, which align perfectly with the requirements for using U-Net with Conv1D. Leveraging this insight, Selen applied U-Net1D to the ETT series datasets for forecasting. While the results did not surpass current state-of-the-art (SOTA) methods, they outperformed several 2022 baselines. Consequently, the project was made publicly available. Selen is eager to encourage collaborative efforts to enhance this project with supplementary experiments and auxiliary measures, exploring the potential for U-pre to evolve into a new SOTA solution. 

# U-pre v2

A significant limitation of U-pre lies in Conv1D's inability to capture long-term dependencies, as it focuses solely on short-term relationships within each window. However, this is not without value, as it complements transformers by addressing local temporal details that might be overlooked in their focus on long-range dependencies. To overcome this limitation, U-pre V2 introduces an innovative hybrid approach that integrates the output of a U-Net convolutional encoder with the original time series as input tokens to a single BERT encoder layer. This approach leverages both local and global contextual information. Furthermore, the U-Net's preprocessing stage implicitly assigns local gradient weights to different timesteps, enabling a more nuanced attention mechanism that transcends the self-attention limitations of traditional sequence models, thereby capturing richer spatio-temporal dependencies.